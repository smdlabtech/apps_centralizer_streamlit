#-----------#
 Question 1: 
#-----------#

You are working on optimizing BigQuery for a query that is run repeatedly on a single table. 
The data queried is about 1 GB, and some rows are expected to change about 10 times every hour. 
You have optimized the SQL statements as much as possible. You want to further optimize the query's performance. 
What should you do?

A. Create a materialized view based on the table, and query that view.
B. Enable caching of the queried data so that subsequent queries are faster.
C. Create a scheduled query, and run it a few minutes before the report has to be created.
D. Reserve a larger number of slots in advance so that you have maximum compute power to execute the query.

My Choice : A.
The correct answer is : ?



#-----------#
 Question 2: 
#-----------#
Several years ago, you built a machine learning model for an ecommerce company. 
Your model made good predictions. Then a global pandemic occurred, lockdowns were imposed, and many people started working from home.
Now the quality of your model has degraded. You want to improve the quality of your model and prevent future performance degradation. 
What should you do?

A. Retrain the model with data from the first 30 days of the lockdown.
B. Monitor data until usage patterns normalize, and then retrain the model.
C. Retrain the model with data from the last 30 days. After one year, return to the older model.
D. Retrain the model with data from the last 30 days. Add a step to continuously monitor model input data for changes, and retrain the model.


My Choice : D.
The correct answer is : ?




#-----------#
 Question 3: 
#-----------#

A new member of your development team works remotely. 
The developer will write code locally on their laptop, which will connect to a MySQL instance on Cloud SQL. 
The instance has an external (public) IP address. You want to follow Google-recommended practices when you give access to Cloud SQL to the new team member. 
What should you do?

A. Ask the developer for their laptop's IP address, and add it to the authorized networks list.
B. Remove the external IP address, and replace it with an internal IP address. Add only the IP address for the remote developer's laptop to the authorized list.
C. Give instance access permissions in Identity and Access Management (IAM), and have the developer run Cloud SQL Auth proxy to connect to a MySQL instance.
D. Give instance access permissions in Identity and Access Management (IAM), change the access to "private service access" for security, and allow the developer to access Cloud SQL from their laptop.

My Choice :
The correct answer is : ?




#-----------#
 Question 4: 
#-----------#

Your Cloud Spanner database stores customer address information that is frequently accessed by the marketing team. 
When a customer enters the country and the state where they live, this information is stored in different tables connected by a foreign key. 
The current architecture has performance issues. 
You want to follow Google-recommended practices to improve performance. 
What should you do?


A. Create interleaved tables, and store states under the countries.
B. Denormalize the data, and have a row for each state with its corresponding country.
C. Retain the existing architecture, but use short, two-letter codes for the countries and states.
D. Combine the countries in a single cell's text, for example "country:state1,state2, â€¦" and when required, split the data.

My Choice : B.
The correct answer is : ?



#-----------#
 Question 5: 
#-----------#

Your company runs its business-critical system on PostgreSQL. 
The system is accessed simultaneously from many locations around the world and supports millions of customers. 
Your database administration team manages the redundancy and scaling manually. 
You want to migrate the database to Google Cloud. You need a solution that will provide global scale and availability and require minimal maintenance. 
What should you do?

A. Migrate to BigQuery.
B. Migrate to Cloud Spanner.
C. Migrate to a Cloud SQL for PostgreSQL instance.
D. Migrate to bare metal machines with PostgreSQL installed.

My Choice : B.
The correct answer is : ?




#-----------#
 Question 6: 
#-----------#

Your company collects data about customers to regularly check their health vitals. 
You have millions of customers around the world. Data is ingested at an average rate of two events per 10 seconds per user. 
You need to be able to visualize data in Bigtable on a per user basis. 
You need to construct the Bigtable key so that the operations are performant. 
What should you do?

A. Construct the key as user-id#device-id#activity-id#timestamp.
B. Construct the key as timestamp#user-id#device-id#activity-id.
C. Construct the key as timestamp#device-id#activity-id#user-id.
D. Construct the key as user-id#timestamp#device-id#activity-id.

My Choice : D.
The correct answer is : ?



#-----------#
 Question 7: 
#-----------#

Your company is hiring several business analysts who are new to BigQuery. 
The analysts will use BigQuery to analyze large quantities of data. 
You need to control costs in BigQuery and ensure that there is no budget overrun while you maintain  the quality of query results. 
What should you do?


A. Set a customized project-level or user-level daily quota to acceptable values.
B. Reduce the data in the BigQuery table so that the analysts query less data, and then archive the remaining data.
C. Train the analysts to use the query validator or --dry_run to estimate costs so that the analysts can self-regulate usage.
D. Export the BigQuery daily costs, and visualize the data on Looker on a per-analyst basis so that the analysts can self-regulate usage.

My Choice :C.
The correct answer is : ?



#-----------#
 Question 8: 
#-----------#

Your Bigtable database was recently deployed into production. 
The scale of data ingested and analyzed has increased significantly, but the performance has degraded. 
You want to identify the performance issue. What should you do?

A. Use Key Visualizer to analyze performance.
B. Use Cloud Trace to identify the performance issue.
C. Add logging statements into the code to see which inserts cause the delay.
D. Add more nodes to the cluster to see if that resolves the performance issue.

My Choice : A.
The correct answer is : ?



#-----------#
 Question 9: 
#-----------#

Your company is moving your data analytics to BigQuery. Your other operations will remain on-premises.
You need to transfer 800 TB of historic data. 
You also need to plan for 30 Gbps of daily data transfers that must be appended for analysis the next day. 
You want to follow Google-recommended practices to transfer your data. 
What should you do?


A. As early as possible every day, use Cloud VPN to transfer the existing data over the internet.
B. Use a Transfer Appliance to move the existing data to Google Cloud. Use Cloud VPN to transfer data daily.
C. Use a Transfer Appliance to move the existing data to Google Cloud.. Use VPC Network Peering to transfer data daily.
D. Use a Transfer Appliance to move the existing data to Google Cloud. Set up a Dedicated or Partner Interconnect for daily transfers.


My Choice : B.
The correct answer is : ?






#-----------#
 Question 10: 
#-----------#

Your team runs Dataproc workloads where the worker node takes about 45 minutes to process. 
You have been exploring various options to optimize the system for cost, including shutting down worker nodes aggressively. 
However, in your metrics you see that the entire job takes even longer. 
You want to optimize the system for cost without increasing job completion time. 
What should you do?


A. Set a graceful decommissioning timeout greater than 45 minutes.
B. Rewrite the processing in Cloud Data Fusion, and run the job automatically.
C. Rewrite the processing in Dataflow, and use stream processing of the same data.
D. Increase the number of vCPUs on each worker node so that the processing finishes sooner.

My Choice : D.
The correct answer is : ?



#-----------#
 Question 11: 
#-----------#

Your customer has a SQL Server database that contains about 5 TB of data in another public cloud. 
You expect the data to grow to a maximum of 25 TB. 
The database is the backend of an internal reporting application that is used once a week. 
You want to migrate the application to Google Cloud to reduce administrative effort while keeping costs the same or reducing them. 
What should you do?


A. Migrate the database to Bigtable.
B. Migrate the database to Cloud Spanner.
C. Install SQL Server on a Compute Engine VM.
D. Migrate the database to SQL Server in Cloud SQL.

My Choice : D.
The correct answer is : ?





#-----------#
 Question 12: 
#-----------#

Your IT team uses BigQuery for storing structured data. 
Your finance team recently moved to Google Workspace Enterprise edition from a standalone, desktop-based spreadsheet processor. 
When the finance team needs data insights, the IT team runs a query on BigQuery, exports the data to a CSV file, and sends the file as an email attachment to the finance team members. 
You want to improve the process while you retain familiar methods of data analysis for the finance team. 
What should you do?


A. Run the query in BigQuery, and give the finance team access to the results view, which can be analyzed.
B. Run the query in BigQuery, and give the finance team access to the data visualizations in Google Data Studio.
C. Run the query in BigQuery, export the data to CSV, upload the file to a Cloud Storage bucket, and share the file with the finance team.
D. Run the query in BigQuery, and save the results to a Google Sheets shared spreadsheet that can be accessed and analyzed by the finance team.


My Choice :B.
The correct answer is : ?




#-----------#
 Question 13: 
#-----------#

Your scooter-sharing company collects information about their scooters, such as location, battery level, and speed. 
The company visualizes this data in real time. 
To guard against intermittent connectivity, each scooter sends repeats of certain messages within a short interval. 
Occasional data errors have been noticed. The messages are received in Pub/Sub and stored in BigQuery. 
You need to ensure that the data does not contain duplicates and that erroneous data with empty fields is rejected. 
What should you do?


A. Store the data in BigQuery, and run delete queries on erroneous and duplicate data.
B. Use Dataflow to subscribe to Pub/Sub, process the data, and store the data in BigQuery.
C. Use Kubernetes to create a microservices application that can remove duplicates and erroneous data. Then insert the data into BigQuery.
D. Create an application on Compute Engine with Managed Instance Groups that can remove duplicates and erroneous data. Then insert the data into BigQuery.

My Choice :B.
The correct answer is : ?


#-----------#
 Question 14: 
#-----------#
Your cryptocurrency trading company visualizes prices to help your customers make trading decisions. 
Because different trades happen in real time, the price data is fed to a data pipeline that uses Dataflow for processing. 
You want to compute moving averages. 
What should you do?


A. Use hopping windows in Dataflow.
B. Use session windows in Dataflow.
C. Use tumbling windows in Dataflow.
D. Use Dataflow SQL, and compute averages grouped by time.

My Choice :D.
The correct answer is : ?




#-----------#
 Question 15: 
#-----------#

You are building the trading platform for a stock exchange with millions of traders. Trading data is written rapidly. 
You need to retrieve data quickly to show visualizations to the traders, such as the changing price of a particular stock over time. 
You need to choose a storage solution in Google Cloud. 
What should you do?


A. Use Bigtable.
B. Use Firestore.
C. Use Cloud SQL.
D. Use Memorystore.

My Choice : B.
The correct answer is : ?



#-----------#
 Question 16: 
#-----------#

Your customer uses Hadoop and Spark to run data analytics on-premises. 
The main data is stored in hard disks that are centrally accessed. 
Your customer needs to migrate their workloads to Google Cloud efficiently while considering scalability. 
You want to select an architecture that requires minimal effort. 
What should you do?


A. Use Dataproc to run Hadoop and Spark jobs. Move the data to Cloud Storage.
B. Use Dataflow to recreate the jobs in a serverless approach. Move the data to Cloud Storage.
C. Use Dataproc to run Hadoop and Spark jobs. Retain the data on a Compute Engine VM with an attached persistent disk.
D. Use Dataflow to recreate the jobs in a serverless approach. Retain the data on a Compute Engine VM with an attached persistent disk.

My Choice : B.
The correct answer is : ?


#-----------#
 Question 17: 
#-----------#





#-----------#
 Question 18: 
#-----------#

You used a small amount of data to build a machine learning model that gives you good inferences during testing. 
However, the results show more errors when real-world data is used to run the model. 
No additional data can be collected for testing. 
You want to get a more accurate view of the model's capability. 
What should you do?


A. Reduce the amount of data to improve the model.
B. Cross-validate the data, and re-run the model building process.
C. Create feature crosses that will add new columns to increase the data.
D. Duplicate the data twice to increase the data, and re-run the model building process


My Choice : D.
The correct answer is : ?


#-----------#
 Question 19: 
#-----------#

Your organization has been collecting information for many years about your customers, including their address and credit card details. 
You plan to use this customer data to build machine learning models on Google Cloud.
You are concerned about private data leaking into the machine learning model. 
Your management is also concerned that direct leaks of personal data could damage the company's reputation. 
You need to address these concerns about data security. 
What should you do?


A. Remove all the tables that contain sensitive data.
B. Use libraries like SciPy to build the ML models on your local computer.
C. Remove the sensitive data by using the Cloud Data Loss Prevention (DLP) API.
D. Identify the rows that contain sensitive data, and apply SQL queries to remove only those row

My Choice : C.
The correct answer is : ?



#-----------#
 Question 20: 
#-----------#

Your healthcare application has a backend system that accepts event data directly from IoT devices. 
Recent increases of the application's users and devices are causing a sudden influx of data that overwhelms the system. 
You need to redesign the data pipeline to ensure that all data is processed and that no events are lost. 
You want to follow Google-recommended practices. 
What should you do? 


A. Use Kafka with pull mode.
B. Use Pub/Sub with pull mode.
C. Use Pub/Sub with push mode.
D. Run Cloud Scheduler at fixed intervals.

My Choice : A.
The correct answer is : ?




#-----------#
 Question 20: 
#-----------#



My Choice :
The correct answer is : ?




#-----------#
 Question 21: 
#-----------#



My Choice :
The correct answer is : ?